{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "import jieba\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch.utils.data as Data\n",
    "import torchtext.vocab as vo\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(tag='train', data_root=\"./data/\"):\n",
    "    \"\"\"\n",
    "    读取数据\n",
    "\n",
    "    参数：\n",
    "        tag: train, test\n",
    "        data_root: 数据集根目录\n",
    "    返回：\n",
    "        data: list, 元素为[review, label]，review为字符串，label为0或1\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    input_file = os.path.join(data_root, \"{0}.csv\".format(tag))\n",
    "    with open(input_file, 'r', encoding=\"utf8\") as f:\n",
    "        head_line = f.readline()\n",
    "        for line in tqdm(f):\n",
    "            line = line.strip()\n",
    "            label = int(line[0])\n",
    "            review = line[2:]\n",
    "            data.append([review, label])  # 评论文本字符串和01标签\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok for tok in jieba.cut(text)]\n",
    "\n",
    "\n",
    "def getTokenized(data):\n",
    "    \"\"\"\n",
    "    获得分词后的数据\n",
    "\n",
    "    参数：\n",
    "        data: list, 元素为[review, label]，review为字符串，label为0或1\n",
    "    返回：\n",
    "        data: list, 元素为[review, label]，review为分词后的list，label为0或1\n",
    "    \"\"\"\n",
    "    return [tokenizer(review) for review, _ in data]\n",
    "\n",
    "\n",
    "def getVocab(tokendata):\n",
    "    \"\"\"\n",
    "    获得词表\n",
    "\n",
    "    参数：\n",
    "        data: list, 元素为[review, label]，review为分词后的list，label为0或1\n",
    "    返回：\n",
    "        vocab: list, 词表\n",
    "    \"\"\"\n",
    "    counter = collections.Counter([tk for st in tokendata for tk in st])\n",
    "    vocabDict = vo.vocab(counter, min_freq=1)\n",
    "    vocabDict.insert_token('<unk>', 1)\n",
    "    vocabDict.insert_token('<pad>', 0)\n",
    "    return vocabDict\n",
    "\n",
    "\n",
    "def pad(text, vocab, maxLen=64):\n",
    "    return text[:maxLen] if len(text) > maxLen else text + [vocab['<pad>']] * (maxLen - len(text))\n",
    "\n",
    "\n",
    "def preProcessData(tokendata, data, vocab, maxLen=64):\n",
    "    \"\"\"\n",
    "    预处理数据\n",
    "\n",
    "    参数：\n",
    "        data: list, 元素为[review, label]，review为分词后的list，label为0或1\n",
    "        vocab: list, 词表\n",
    "        maxLen: int, 最大长度\n",
    "    返回：\n",
    "        data: list, 元素为[review, label]，review为分词后的list，label为0或1\n",
    "    \"\"\"\n",
    "    \n",
    "    padTokenData=[]\n",
    "    for words in tokendata:\n",
    "        indexWord = [vocab[words] if words in vocab else vocab['<unk>'] for words in words]\n",
    "        padWord = pad(indexWord, vocab, maxLen)\n",
    "        padTokenData.append(padWord)\n",
    "    features = torch.tensor(padTokenData)\n",
    "    labels = torch.tensor([label for _, label in data])\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def makeDataset(batchsize=64, maxlength=64, mincount=5):\n",
    "    \"\"\"\n",
    "    生成数据集\n",
    "\n",
    "    参数：\n",
    "        batchsize: int, 批大小\n",
    "        maxlength: int, 最大长度\n",
    "        mincount: int, 最小词频\n",
    "    返回：\n",
    "        train_iter: 训练集迭代器\n",
    "        test_iter: 测试集迭代器\n",
    "        vocab: 词表\n",
    "    \"\"\"\n",
    "    trainData = readData(tag='train')\n",
    "    testData = readData(tag='test')\n",
    "    trainTokenData = getTokenized(trainData)\n",
    "    testTokenData = getTokenized(testData)\n",
    "    vocab = getVocab(trainTokenData)\n",
    "    trainFeatures, trainLabels = preProcessData(trainTokenData, trainData, vocab, maxlength)\n",
    "    testFeatures, testLabels = preProcessData(testTokenData, testData, vocab, maxlength)\n",
    "    train_dataset=Data.TensorDataset(trainFeatures, trainLabels)\n",
    "    test_dataset=Data.TensorDataset(testFeatures, testLabels)\n",
    "    train_loader = Data.DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "    test_loader = Data.DataLoader(test_dataset, batch_size=batchsize, shuffle=True)\n",
    "    return train_loader, test_loader, vocab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def saveVocab(vocab, path='./output/model.vocab'):\n",
    "    \"\"\"\n",
    "    保存词表\n",
    "\n",
    "    参数：\n",
    "        vocab: 词表\n",
    "        path: 保存路径\n",
    "    \"\"\"\n",
    "    with open(path, 'w', encoding=\"utf8\") as output:\n",
    "        print(\"\\n\".join(vocab.get_itos()), file=output)\n",
    "\n",
    "\n",
    "def loadVocab(path='./output/model.vocab'):\n",
    "    \"\"\"\n",
    "    读取词表\n",
    "\n",
    "    参数：\n",
    "        path: 读取路径\n",
    "    返回：\n",
    "        vocab: 词表\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    with open(path, 'r', encoding=\"utf8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            vocab[line.strip()] = i\n",
    "    vocabDict=vo.vocab(vocab, min_freq=0)\n",
    "    return vocabDict\n",
    "\n",
    "\n",
    "def loadPreTrainEmbedding(words,preTrainVocabPath='./data/glove/',embedSize=100,type='glove'):\n",
    "    \"\"\"\n",
    "    加载预训练词向量\n",
    "\n",
    "    参数：\n",
    "        words: 词表\n",
    "        preTrainVocabPath: 预训练词向量路径\n",
    "        embedSize: 词向量维度\n",
    "    返回：\n",
    "        embed: 词向量\n",
    "    \"\"\"\n",
    "    embed = torch.normal(mean=0, std=1, size=(len(words), embedSize))\n",
    "    if type=='glove':\n",
    "        preTrainVocab = vocab.GloVe(name='6B', dim=embedSize, cache=preTrainVocabPath)\n",
    "    elif type=='word2vec':\n",
    "        preTrainVocab = vocab.Word2VecTextFile(preTrainVocabPath)\n",
    "    else:\n",
    "        return embed\n",
    "    preTrainVocabSize = preTrainVocab.vectors.shape[0]\n",
    "    outofvocab = 0\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            index = preTrainVocab.stoi[word]\n",
    "            if preTrainVocabSize == embedSize:\n",
    "                embed[i,:] = preTrainVocab.vectors[index]\n",
    "            elif preTrainVocabSize > embedSize:\n",
    "                embed[i,:] = preTrainVocab.vectors[index][:embedSize]\n",
    "            else:\n",
    "                embed[i,:] = preTrainVocab.vectors[index] + [0] * (embedSize - preTrainVocabSize)\n",
    "        except:\n",
    "            outofvocab += 1\n",
    "    if outofvocab > 0:\n",
    "        print('out of vocab: %d' % outofvocab)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114026it [00:00, 340723.57it/s]\n",
      "5962it [00:00, 424579.19it/s]\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\76608\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.789 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, vocab_dict=makeDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveVocab(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, bidirectional, num_classes, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        初始化模型\n",
    "\n",
    "        参数：\n",
    "            vocab_size: int, 词表大小\n",
    "            embed_size: int, 词向量维度\n",
    "            num_hiddens: int, 隐藏层维度\n",
    "            num_layers: int, 隐藏层层数\n",
    "            bidirectional: bool, 是否双向\n",
    "            num_classes: int, 类别数\n",
    "            drop_prob: float, dropout概率\n",
    "\n",
    "        \"\"\"\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.encoder = nn.LSTM(input_size=embed_size,\n",
    "                               hidden_size=num_hiddens,\n",
    "                               num_layers=num_layers,\n",
    "                               bidirectional=bidirectional,\n",
    "                               dropout=drop_prob)\n",
    "        self.decoder = nn.Linear(4 * num_hiddens, num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        参数：\n",
    "            inputs: Tensor, 输入\n",
    "        返回：\n",
    "            outputs: Tensor, 输出\n",
    "        \"\"\"\n",
    "        inputs=inputs.permute(1,0)\n",
    "        # torch.Size([64, 64])\n",
    "        embeddings = self.embedding(inputs)\n",
    "        # torch.Size([64, 64, 64])\n",
    "        outputs, _ = self.encoder(embeddings)\n",
    "        # torch.Size([64, 64, 128])\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "        # torch.Size([64, 256])\n",
    "        outs = self.decoder(encoding)\n",
    "        # torch.Size([64, 2])\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel():\n",
    "    \"\"\"\n",
    "    测试模型\n",
    "    \"\"\"\n",
    "    model = BiRNN(vocab_size=5000, embed_size=100, num_hiddens=100, num_layers=2, bidirectional=True, num_classes=2)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiRNN(\n",
      "  (embedding): Embedding(5000, 100)\n",
      "  (encoder): LSTM(100, 100, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (decoder): Linear(in_features=400, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "testModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准确率\n",
    "def accuracy(data_iter, model, device):\n",
    "    \"\"\"\n",
    "    计算准确率\n",
    "\n",
    "    参数：\n",
    "        data_iter: DataLoader, 数据迭代器\n",
    "        model: 模型\n",
    "        device: 设备\n",
    "    返回：\n",
    "        acc: float, 准确率\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            acc_sum += (model(X).argmax(dim=1) == y).float().sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, test_loader, model, loss, optimizer, device, epochs):\n",
    "    model = model.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    trainLoss = []\n",
    "    trainAcc = []\n",
    "    testAcc = []\n",
    "    for epoch in range(epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        model.train()\n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = accuracy(test_loader, model, device)\n",
    "        trainLoss.append(train_l_sum / batch_count)\n",
    "        trainAcc.append(train_acc_sum / n)\n",
    "        testAcc.append(test_acc)\n",
    "        # print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "            #   % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
    "    plt.plot(np.arange(epochs), trainLoss, label='train loss')\n",
    "    plt.plot(np.arange(epochs), trainAcc, label='train acc')\n",
    "    plt.plot(np.arange(epochs), testAcc, label='test acc')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiRNN(\n",
      "  (embedding): Embedding(196816, 64)\n",
      "  (encoder): LSTM(64, 64, num_layers=2, dropout=0.9, bidirectional=True)\n",
      "  (decoder): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embed_size = 64\n",
    "num_hiddens = 64\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "num_classes = 2\n",
    "drop_prob = 0.9\n",
    "lr = 0.01\n",
    "epochs = 2\n",
    "net = BiRNN(vocab_size=len(vocab_dict), embed_size=embed_size, num_hiddens=num_hiddens, num_layers=num_layers, bidirectional=bidirectional, num_classes=num_classes, drop_prob=drop_prob)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(train_loader, test_loader, net, loss, optimizer, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputModelPath='output/model.pt'\n",
    "torch.save(net, outputModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    \"\"\"\n",
    "    清洗文本\n",
    "\n",
    "    参数：\n",
    "        text: str, 文本\n",
    "    返回：\n",
    "        text: str, 清洗后的文本\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def makeIter(texts, vocab_path='output/model.vocab', batch_size=64, max_length=64):\n",
    "    \"\"\"\n",
    "    生成迭代器\n",
    "\n",
    "    参数：\n",
    "        texts: list, 文本列表\n",
    "        vocab_path: str, 词表路径\n",
    "        batch_size: int, 批大小\n",
    "        max_length: int, 最大长度\n",
    "    返回：\n",
    "        data_iter: DataLoader, 数据迭代器\n",
    "    \"\"\"\n",
    "    data=[]\n",
    "    for text in texts:\n",
    "        text=cleanText(text)\n",
    "        data.append([text,-1])\n",
    "    vocab=loadVocab(vocab_path)\n",
    "    tokenizedData = getTokenized(data)\n",
    "    features, labels = preProcessData(tokenizedData, data, vocab, max_length)\n",
    "    dataset = Data.TensorDataset(features, labels)\n",
    "    data_iter = Data.DataLoader(dataset, batch_size, shuffle=False)\n",
    "    return data_iter\n",
    "\n",
    "\n",
    "def makeIterOfFile(filePath='data/infer.txt', vocab_path='output/model.vocab', batch_size=64, max_length=64):\n",
    "    \"\"\"\n",
    "    生成迭代器\n",
    "\n",
    "    参数：\n",
    "        filePath: str, 文件路径\n",
    "        vocab_path: str, 词表路径\n",
    "        batch_size: int, 批大小\n",
    "        max_length: int, 最大长度\n",
    "    返回：\n",
    "        data_iter: DataLoader, 数据迭代器\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filePath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = cleanText(line)\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data.append([line, -1])\n",
    "    vocab = loadVocab(vocab_path)\n",
    "    tokenizedData = getTokenized(data)\n",
    "    features, labels = preProcessData(tokenizedData, data, vocab, max_length)\n",
    "    dataset = Data.TensorDataset(features, labels)\n",
    "    data_iter = Data.DataLoader(dataset, batch_size, shuffle=False)\n",
    "    return data_iter, vocab\n",
    "\n",
    "def inference(text_iter, device, outputModelPath='output/model.pt'):\n",
    "    \"\"\"\n",
    "    预测\n",
    "\n",
    "    参数：\n",
    "        text: str, 文本\n",
    "        outputModelPath: str, 模型路径\n",
    "    返回：\n",
    "        label: int, 类别\n",
    "    \"\"\"\n",
    "    model = torch.load(outputModelPath)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    result=[]\n",
    "    with torch.no_grad():\n",
    "        for X, y in text_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(X)\n",
    "            temp = y_hat.argmax(dim=1).cpu().numpy()\n",
    "            result.extend(temp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_iter, vocab = makeIterOfFile()\n",
    "print(\"#vocab: \", len(vocab))\n",
    "print('#batches:', len(text_iter))\n",
    "result = inference(text_iter, device)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext = ['I love54@@@@56465   $#%#   $@$@$@ sd fdf you','I hate you','You are so stupid','I am so happy','I am so sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataIter=makeIter(testtext)\n",
    "result=inference(dataIter,device)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2e8ddb4de15988e6e8d3645e3fa3d3cd833fd38eaa27b2aef6b9f7c4ad9c031"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
