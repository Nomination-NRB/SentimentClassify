{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "import jieba\n",
    "import torch\n",
    "import collections\n",
    "import torch.utils.data as Data\n",
    "import torchtext.vocab as vo\n",
    "from tqdm import tqdm\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(tag='train', data_root=\"./data/\"):\n",
    "    \"\"\"\n",
    "    读取数据\n",
    "\n",
    "    参数：\n",
    "        tag: train, test\n",
    "        data_root: 数据集根目录\n",
    "    返回：\n",
    "        data: list, 元素为[review, label]，review为字符串，label为0或1\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    input_file = os.path.join(data_root, \"{0}.csv\".format(tag))\n",
    "    with open(input_file, 'r', encoding=\"utf8\") as f:\n",
    "        head_line = f.readline()\n",
    "        for line in tqdm(f):\n",
    "            line = line.strip()\n",
    "            label = int(line[0])\n",
    "            review = line[2:]\n",
    "            data.append([review, label])  # 评论文本字符串和01标签\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok for tok in jieba.cut(text)]\n",
    "\n",
    "\n",
    "def getTokenized(data):\n",
    "    \"\"\"\n",
    "    获得分词后的数据\n",
    "\n",
    "    参数：\n",
    "        data: list, 元素为[review, label]，review为字符串，label为0或1\n",
    "    返回：\n",
    "        data: list, 元素为[review, label]，review为分词后的list，label为0或1\n",
    "    \"\"\"\n",
    "    return [tokenizer(review) for review, _ in data]\n",
    "\n",
    "\n",
    "def getVocab(tokendata):\n",
    "    \"\"\"\n",
    "    获得词表\n",
    "\n",
    "    参数：\n",
    "        data: list, 元素为[review, label]，review为分词后的list，label为0或1\n",
    "    返回：\n",
    "        vocab: list, 词表\n",
    "    \"\"\"\n",
    "    counter = collections.Counter([tk for st in tokendata for tk in st])\n",
    "    vocabDict = vo.vocab(counter, min_freq=1)\n",
    "    vocabDict.insert_token('<unk>', 1)\n",
    "    vocabDict.insert_token('<pad>', 0)\n",
    "    return vocabDict\n",
    "\n",
    "\n",
    "def pad(text, vocab, maxLen=64):\n",
    "    return text[:maxLen] if len(text) > maxLen else text + [vocab['<pad>']] * (maxLen - len(text))\n",
    "\n",
    "\n",
    "def preProcessData(tokendata, data, vocab, maxLen=64):\n",
    "    \"\"\"\n",
    "    预处理数据\n",
    "\n",
    "    参数：\n",
    "        data: list, 元素为[review, label]，review为分词后的list，label为0或1\n",
    "        vocab: list, 词表\n",
    "        maxLen: int, 最大长度\n",
    "    返回：\n",
    "        data: list, 元素为[review, label]，review为分词后的list，label为0或1\n",
    "    \"\"\"\n",
    "    \n",
    "    padTokenData=[]\n",
    "    for words in tokendata:\n",
    "        indexWord = [vocab[words] if words in vocab else vocab['<unk>'] for words in words]\n",
    "        padWord = pad(indexWord, vocab, maxLen)\n",
    "        padTokenData.append(padWord)\n",
    "    features = torch.tensor(padTokenData)\n",
    "    labels = torch.tensor([label for _, label in data])\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def makeDataset(batchsize=64, maxlength=64, mincount=5):\n",
    "    \"\"\"\n",
    "    生成数据集\n",
    "\n",
    "    参数：\n",
    "        batchsize: int, 批大小\n",
    "        maxlength: int, 最大长度\n",
    "        mincount: int, 最小词频\n",
    "    返回：\n",
    "        train_iter: 训练集迭代器\n",
    "        test_iter: 测试集迭代器\n",
    "        vocab: 词表\n",
    "    \"\"\"\n",
    "    trainData = readData(tag='train')\n",
    "    testData = readData(tag='test')\n",
    "    trainTokenData = getTokenized(trainData)\n",
    "    testTokenData = getTokenized(testData)\n",
    "    vocab = getVocab(trainTokenData)\n",
    "    trainFeatures, trainLabels = preProcessData(trainTokenData, trainData, vocab, maxlength)\n",
    "    testFeatures, testLabels = preProcessData(testTokenData, testData, vocab, maxlength)\n",
    "    train_dataset=Data.TensorDataset(trainFeatures, trainLabels)\n",
    "    test_dataset=Data.TensorDataset(testFeatures, testLabels)\n",
    "    train_loader = Data.DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "    test_loader = Data.DataLoader(test_dataset, batch_size=batchsize, shuffle=True)\n",
    "    return train_loader, test_loader, vocab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def saveVocab(vocab, path='./output/model.vocab'):\n",
    "    \"\"\"\n",
    "    保存词表\n",
    "\n",
    "    参数：\n",
    "        vocab: 词表\n",
    "        path: 保存路径\n",
    "    \"\"\"\n",
    "    with open(path, 'w', encoding=\"utf8\") as output:\n",
    "        print(\"\\n\".join(vocab.get_itos()), file=output)\n",
    "\n",
    "\n",
    "def loadVocab(path='./output/model.vocab'):\n",
    "    \"\"\"\n",
    "    读取词表\n",
    "\n",
    "    参数：\n",
    "        path: 读取路径\n",
    "    返回：\n",
    "        vocab: 词表\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    with open(path, 'r', encoding=\"utf8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            vocab[line.strip()] = i\n",
    "    vocabDict=vo.vocab(vocab, min_freq=0)\n",
    "    return vocabDict\n",
    "\n",
    "\n",
    "def loadPreTrainEmbedding(words,preTrainVocabPath='./data/glove/',embedSize=100,type='glove'):\n",
    "    \"\"\"\n",
    "    加载预训练词向量\n",
    "\n",
    "    参数：\n",
    "        words: 词表\n",
    "        preTrainVocabPath: 预训练词向量路径\n",
    "        embedSize: 词向量维度\n",
    "    返回：\n",
    "        embed: 词向量\n",
    "    \"\"\"\n",
    "    embed = torch.normal(mean=0, std=1, size=(len(words), embedSize))\n",
    "    if type=='glove':\n",
    "        preTrainVocab = vocab.GloVe(name='6B', dim=embedSize, cache=preTrainVocabPath)\n",
    "    elif type=='word2vec':\n",
    "        preTrainVocab = vocab.Word2VecTextFile(preTrainVocabPath)\n",
    "    else:\n",
    "        return embed\n",
    "    preTrainVocabSize = preTrainVocab.vectors.shape[0]\n",
    "    outofvocab = 0\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            index = preTrainVocab.stoi[word]\n",
    "            if preTrainVocabSize == embedSize:\n",
    "                embed[i,:] = preTrainVocab.vectors[index]\n",
    "            elif preTrainVocabSize > embedSize:\n",
    "                embed[i,:] = preTrainVocab.vectors[index][:embedSize]\n",
    "            else:\n",
    "                embed[i,:] = preTrainVocab.vectors[index] + [0] * (embedSize - preTrainVocabSize)\n",
    "        except:\n",
    "            outofvocab += 1\n",
    "    if outofvocab > 0:\n",
    "        print('out of vocab: %d' % outofvocab)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114026it [00:00, 276090.82it/s]\n",
      "5962it [00:00, 372724.22it/s]\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\76608\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.007 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, vocab_dict=makeDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveVocab(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, bidirectional, num_classes, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        初始化模型\n",
    "\n",
    "        参数：\n",
    "            vocab_size: int, 词表大小\n",
    "            embed_size: int, 词向量维度\n",
    "            num_hiddens: int, 隐藏层维度\n",
    "            num_layers: int, 隐藏层层数\n",
    "            bidirectional: bool, 是否双向\n",
    "            num_classes: int, 类别数\n",
    "            drop_prob: float, dropout概率\n",
    "\n",
    "        \"\"\"\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.encoder = nn.LSTM(input_size=embed_size,\n",
    "                               hidden_size=num_hiddens,\n",
    "                               num_layers=num_layers,\n",
    "                               bidirectional=bidirectional,\n",
    "                               dropout=drop_prob)\n",
    "        self.decoder = nn.Linear(4 * num_hiddens, num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        参数：\n",
    "            inputs: Tensor, 输入\n",
    "        返回：\n",
    "            outputs: Tensor, 输出\n",
    "        \"\"\"\n",
    "        inputs=inputs.permute(1,0)\n",
    "        # torch.Size([64, 64])\n",
    "        embeddings = self.embedding(inputs)\n",
    "        # torch.Size([64, 64, 64])\n",
    "        outputs, _ = self.encoder(embeddings)\n",
    "        # torch.Size([64, 64, 128])\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "        # torch.Size([64, 256])\n",
    "        outs = self.decoder(encoding)\n",
    "        # torch.Size([64, 2])\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel():\n",
    "    \"\"\"\n",
    "    测试模型\n",
    "    \"\"\"\n",
    "    model = BiRNN(vocab_size=5000, embed_size=100, num_hiddens=100, num_layers=2, bidirectional=True, num_classes=2)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiRNN(\n",
      "  (embedding): Embedding(5000, 100)\n",
      "  (encoder): LSTM(100, 100, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (decoder): Linear(in_features=400, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "testModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准确率\n",
    "def accuracy(data_iter, model, device):\n",
    "    \"\"\"\n",
    "    计算准确率\n",
    "\n",
    "    参数：\n",
    "        data_iter: DataLoader, 数据迭代器\n",
    "        model: 模型\n",
    "        device: 设备\n",
    "    返回：\n",
    "        acc: float, 准确率\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            acc_sum += (model(X).argmax(dim=1) == y).float().sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, test_loader, model, loss, optimizer, device, epochs):\n",
    "    model = model.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        model.train()\n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = accuracy(test_loader, model, device)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiRNN(\n",
      "  (embedding): Embedding(196816, 64)\n",
      "  (encoder): LSTM(64, 64, num_layers=2, dropout=0.9, bidirectional=True)\n",
      "  (decoder): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embed_size = 64\n",
    "num_hiddens = 64\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "num_classes = 2\n",
    "drop_prob = 0.9\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "net = BiRNN(vocab_size=len(vocab_dict), embed_size=embed_size, num_hiddens=num_hiddens, num_layers=num_layers, bidirectional=bidirectional, num_classes=num_classes, drop_prob=drop_prob)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.1791, train acc 0.929, test acc 0.951, time 128.7 sec\n",
      "epoch 2, loss 0.0771, train acc 0.942, test acc 0.941, time 133.0 sec\n",
      "epoch 3, loss 0.0714, train acc 0.917, test acc 0.940, time 131.0 sec\n",
      "epoch 4, loss 0.0500, train acc 0.926, test acc 0.950, time 129.9 sec\n",
      "epoch 5, loss 0.0409, train acc 0.926, test acc 0.942, time 127.4 sec\n",
      "epoch 6, loss 0.0522, train acc 0.871, test acc 0.881, time 129.7 sec\n",
      "epoch 7, loss 0.0597, train acc 0.808, test acc 0.829, time 128.4 sec\n",
      "epoch 8, loss 0.0566, train acc 0.786, test acc 0.740, time 126.5 sec\n",
      "epoch 9, loss 0.0546, train acc 0.762, test acc 0.804, time 132.3 sec\n",
      "epoch 10, loss 0.0517, train acc 0.745, test acc 0.760, time 139.6 sec\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, test_loader, net, loss, optimizer, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputModelPath='output/model.pt'\n",
    "torch.save(net, outputModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    \"\"\"\n",
    "    清洗文本\n",
    "\n",
    "    参数：\n",
    "        text: str, 文本\n",
    "    返回：\n",
    "        text: str, 清洗后的文本\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeIter(texts, vocab_path='output/model.vocab', batch_size=64, max_length=64):\n",
    "    \"\"\"\n",
    "    生成迭代器\n",
    "\n",
    "    参数：\n",
    "        texts: list, 文本列表\n",
    "        vocab_path: str, 词表路径\n",
    "        batch_size: int, 批大小\n",
    "        max_length: int, 最大长度\n",
    "    返回：\n",
    "        data_iter: DataLoader, 数据迭代器\n",
    "    \"\"\"\n",
    "    data=[]\n",
    "    for text in texts:\n",
    "        text=cleanText(text)\n",
    "        data.append([text,-1])\n",
    "    vocab=loadVocab(vocab_path)\n",
    "    tokenizedData = getTokenized(data)\n",
    "    features, labels = preProcessData(tokenizedData, data, vocab, max_length)\n",
    "    dataset = Data.TensorDataset(features, labels)\n",
    "    data_iter = Data.DataLoader(dataset, batch_size, shuffle=False)\n",
    "    return data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text_iter, device, outputModelPath='output/model.pt'):\n",
    "    \"\"\"\n",
    "    预测\n",
    "\n",
    "    参数：\n",
    "        text: str, 文本\n",
    "        outputModelPath: str, 模型路径\n",
    "    返回：\n",
    "        label: int, 类别\n",
    "    \"\"\"\n",
    "    model = torch.load(outputModelPath)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    result=[]\n",
    "    with torch.no_grad():\n",
    "        for X, y in text_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(X)\n",
    "            temp = y_hat.argmax(dim=1).cpu().numpy()\n",
    "            result.extend(temp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext = ['I love54@@@@56465   $#%#   $@$@$@ sd fdf you','I hate you','You are so stupid','I am so happy','I am so sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataIter=makeIter(testtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=inference(dataIter,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15 (default, Dec  3 2021, 18:25:24) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2e8ddb4de15988e6e8d3645e3fa3d3cd833fd38eaa27b2aef6b9f7c4ad9c031"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
