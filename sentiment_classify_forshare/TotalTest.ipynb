{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import jieba\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torchtext.vocab as vocab\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab(vocab, path):\n",
    "    # print(vocab.get_itos())\n",
    "    with open(path, 'w', encoding=\"utf8\") as output:\n",
    "        print(\"\\n\".join(vocab.get_itos()), file=output)\n",
    "\n",
    "def read_vocab(vocab_path):\n",
    "    vocab_dict = {}\n",
    "    with open(vocab_path, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word = line[:-1]\n",
    "            # print(\"*{0}*\".format(word))\n",
    "            if word == \"\": continue\n",
    "            vocab_dict[word] = 1\n",
    "    dict = vocab.vocab(vocab_dict, min_freq=0)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_imdb(folder='train', data_root=\"data\"):\n",
    "    data = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_name = os.path.join(data_root, folder, label)\n",
    "        print(folder_name)\n",
    "        for file in tqdm(os.listdir(folder_name)):\n",
    "            input_file = os.path.join(folder_name, file)\n",
    "            print(\"reading file {0}\".format(input_file), file=sys.stderr)\n",
    "            with open(input_file, 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '').lower()\n",
    "                data.append([review, 1 if label == 'pos' else 0])  # 评论文本字符串和01标签\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "def get_tokenized_imdb(data):  # 将每行数据的进行空格切割,保留每个的单词\n",
    "    \"\"\"\n",
    "    @params:\n",
    "        data: 数据的列表，列表中的每个元素为 [文本字符串，0/1标签] 二元组\n",
    "    @return: 切分词后的文本的列表，列表中的每个元素为切分后的词序列\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(' ')]\n",
    "\n",
    "    return [tokenizer(review) for review, _ in data]\n",
    "\n",
    "def get_vocab_imdb(data, min_count=1):\n",
    "    \"\"\"\n",
    "    @params:\n",
    "        data: 同上\n",
    "    @return: 数据集上的词典，Vocab 的实例（freqs, stoi, itos）\n",
    "    \"\"\"\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    # print(counter)\n",
    "    # 统计所有的数据\n",
    "    dict = vocab.vocab(counter, min_freq=min_count)  # 构建词汇表\n",
    " \n",
    "    # 加入<pad> 和 <unk>\n",
    "    dict.insert_token(\"<pad>\", 0)\n",
    "    dict.insert_token(\"<unk>\", 1)\n",
    "    return dict\n",
    "\n",
    "def preprocess_imdb(data, vocab, max_l=128):\n",
    "    \"\"\"\n",
    "    @params:\n",
    "        data: 同上，原始的读入数据\n",
    "        vocab: 训练集上生成的词典\n",
    "    @return:\n",
    "        features: 单词下标序列，形状为 (n, max_l) 的整数张量\n",
    "        labels: 情感标签，形状为 (n,) 的0/1整数张量\n",
    "    \"\"\"\n",
    "\n",
    "    def pad(x):  # 填充\n",
    "        return x[:max_l] if len(x) > max_l else x + [vocab[\"<pad>\"]] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    # pprint(tokenized_data[:10])\n",
    "    padded_tokenized_data = []\n",
    "    for words in tokenized_data:\n",
    "        indexed_words = [vocab[word] if word in vocab else vocab[\"<unk>\"] for word in words]\n",
    "        padded_words = pad(indexed_words)\n",
    "        padded_tokenized_data.append(padded_words)\n",
    "    # pprint(padded_tokenized_data[:10])\n",
    "    features = torch.tensor(padded_tokenized_data)\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels\n",
    "\n",
    "def make_imdb_dataset(batch_size=64, max_length=128, min_count=5):\n",
    "    # 读取文本数据\n",
    "    train_data, test_data = read_imdb(folder=\"train\"), read_imdb(folder=\"test\")\n",
    "    # 获取字典\n",
    "    vocab = get_vocab_imdb(train_data, min_count)\n",
    "    # *号语法糖,解绑参数，获取dataset对象\n",
    "    train_set = Data.TensorDataset(*preprocess_imdb(train_data, vocab, max_length))\n",
    "    test_set = Data.TensorDataset(*preprocess_imdb(test_data, vocab, max_length))  # 相当于将函数参数是函数结果\n",
    "    # 获取迭代器\n",
    "    train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "    test_iter = Data.DataLoader(test_set, batch_size)\n",
    "\n",
    "    return train_iter, test_iter, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_weibo(tag='training-processed', data_root=\"data\\\\weibo_senti_100k\"):\n",
    "    data = []\n",
    "    input_file = os.path.join(data_root, \"{0}.csv\".format(tag))\n",
    "    with open(input_file, 'r', encoding=\"utf8\") as f:\n",
    "        for line in tqdm(f):\n",
    "            line = line.strip()\n",
    "            # print(\"line: \",line)\n",
    "            # print(len(line))\n",
    "            i = 0\n",
    "            j = 0\n",
    "            for i in range(len(line)):\n",
    "                if line[i] == ',':\n",
    "                    j+=1\n",
    "                if j == 5:\n",
    "                    break\n",
    "            if int(line[0]) == 4:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = int(line[0])\n",
    "            # print(i)\n",
    "            review = line[i+1:-5]\n",
    "            # print(review)\n",
    "            data.append([review, label])  # 评论文本字符串和01标签\n",
    "    random.shuffle(data)\n",
    "    # print(data[:10])\n",
    "    return data\n",
    "\n",
    "def get_tokenized_weibo(data):  # 将每行数据的进行空格切割,保留每个的单词\n",
    "    # 此处可以添加更复杂的过滤逻辑\n",
    "    def tokenizer(text):\n",
    "        return [tok for tok in jieba.cut(text)]\n",
    "\n",
    "    mylist = [tokenizer(review) for review, _ in data]\n",
    "    # print(mylist[:10])\n",
    "    return mylist\n",
    "\n",
    "def get_vocab_weibo(data, min_count=1):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 同上\n",
    "    @return: 数据集上的词典，Vocab 的实例（freqs, stoi, itos）\n",
    "    '''\n",
    "    tokenized_data = get_tokenized_weibo(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    # print(counter)\n",
    "    # 统计所有的数据\n",
    "    dict = vocab.vocab(counter, min_freq=min_count)  # 构建词汇表\n",
    "    # 加入<pad> 和 <unk>\n",
    "    dict.insert_token(\"<pad>\", 0)\n",
    "    dict.insert_token(\"<unk>\", 1)\n",
    "    return dict\n",
    "\n",
    "def preprocess_weibo(data, vocab, max_l=64):\n",
    "    def pad(x):  # 填充\n",
    "        return x[:max_l] if len(x) > max_l else x + [vocab[\"<pad>\"]] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = get_tokenized_weibo(data)\n",
    "    # print(tokenized_data[:10])\n",
    "    # print(\"*****************\")\n",
    "    # print(vocab.get_stoi())\n",
    "    padded_tokenized_data = []\n",
    "    for words in tokenized_data:\n",
    "        indexed_words = [vocab[word] if word in vocab else vocab[\"<unk>\"] for word in words]\n",
    "        # print(\"indexed_words: \",indexed_words)\n",
    "        padded_words = pad(indexed_words)\n",
    "        # print(\"padded_words: \",padded_words)\n",
    "        padded_tokenized_data.append(padded_words)\n",
    "        # print(\"padded_tokenized_data: \",padded_tokenized_data)\n",
    "    # print(padded_tokenized_data[:10])\n",
    "    features = torch.tensor(padded_tokenized_data)\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels\n",
    "\n",
    "def make_weibo_dataset(batch_size=64, max_length=64, min_count=5):\n",
    "    # 读取文本数据\n",
    "    train_data, test_data = read_weibo(tag=\"test_sample\"), read_weibo(tag=\"test_sample2\")\n",
    "    # 获取字典\n",
    "    vocab = get_vocab_weibo(train_data, min_count)\n",
    "    # *号语法糖,解绑参数，获取dataset对象\n",
    "    train_set = Data.TensorDataset(*preprocess_weibo(train_data, vocab, max_length))\n",
    "    test_set = Data.TensorDataset(*preprocess_weibo(test_data, vocab, max_length)) \n",
    "    # 获取迭代器\n",
    "    train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "    test_iter = Data.DataLoader(test_set, batch_size)\n",
    "\n",
    "    return train_iter, test_iter, vocab\n",
    "\n",
    "# 用已经训练好的模型来标记\n",
    "def make_weibo_testset(file_path, vocab_path, batch_size=64, max_length=64):\n",
    "    # 读取数据\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            review = line.strip()\n",
    "            label = 0  # 标签固定为0\n",
    "            data.append([review, label])\n",
    "\n",
    "    # 读入词典\n",
    "    vocab = read_vocab(vocab_path)\n",
    "    # *号语法糖,解绑参数，获取dataset对象\n",
    "    data_set = Data.TensorDataset(*preprocess_weibo(data, vocab, max_length))  # 相当于将函数参数是函数结果\n",
    "    # 获取迭代器\n",
    "    data_iter = Data.DataLoader(data_set, batch_size)\n",
    "\n",
    "    return data_iter, vocab\n",
    "\n",
    "def load_pretrained_embedding(words, pretrained_vocab_path=None, emb_size=100, type=\"glove\"):\n",
    "    '''\n",
    "    @params:\n",
    "        words: 需要加载词向量的词语列表，以 itos (index to string) 的词典形式给出\n",
    "        pretrained_vocab: 预训练词向量\n",
    "        type: 词向量的种类\n",
    "    @return:\n",
    "        embed: 加载到的词向量\n",
    "    '''\n",
    "    # embed = torch.zeros(len(words), emb_size)  # 初始化为len*100维度\n",
    "    embed = torch.normal(mean=0, std=1, size=(len(words), emb_size))\n",
    "    if type == \"glove\":\n",
    "        # 先硬编码使用100d的glove向量\n",
    "        pretrained_vocab = vocab.GloVe(name=\"6B\", dim=100, cache=\"data\\\\glove\")\n",
    "    else:\n",
    "        return embed\n",
    "\n",
    "    pretrained_emb_size = pretrained_vocab.vectors[0].shape[0]\n",
    "    oov_count = 0  # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            if pretrained_emb_size == emb_size:\n",
    "                embed[i, :] = pretrained_vocab.vectors[idx]  # 将每个词语用训练的语言模型理解\n",
    "            elif pretrained_emb_size < emb_size:\n",
    "                embed[1, :] = pretrained_vocab.vectors[idx] + [0] * (emb_size - pretrained_emb_size)\n",
    "            else:\n",
    "                embed[1, :] = pretrained_vocab.vectors[idx][:emb_size]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    # print(embed.shape),在词典中寻找相匹配的词向量\n",
    "    return embed\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, test_data = read_weibo(tag=\"training-processed\"), read_weibo(tag=\"test_sample\")\n",
    "train_data, test_data = read_weibo(tag=\"test_sample\"), read_weibo(tag=\"test_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in train_data[:5]:\n",
    "    #     print(sample[1], '\\t', sample[0][:])\n",
    "for review,i in train_data[:5]:\n",
    "    print(i, '\\t', review[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = get_tokenized_weibo(train_data[:5])\n",
    "print(tokenized_data[:5])\n",
    "print('# words in vocab:', len(tokenized_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "print('counter:', counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = vocab.vocab(counter)  # 构建词汇表\n",
    "temp=dict\n",
    "print((temp.get_itos()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 1\n",
    "dict=get_vocab_weibo(train_data[:5], min_count)\n",
    "print(\"Vocab size: {0}\".format(len(dict)))\n",
    "print(dict.get_itos())\n",
    "print(dict.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, lable = preprocess_weibo(train_data[:2], dict, max_l=5)\n",
    "# print(features.size())\n",
    "# print(lable.size())\n",
    "\n",
    "print(features[:2])\n",
    "print(lable[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"data\\\\glove\"\n",
    "glove_vocab = vocab.GloVe(name='6B', dim=100, cache=cache_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained_embedding(vocab.get_itos(), glove_vocab)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2e8ddb4de15988e6e8d3645e3fa3d3cd833fd38eaa27b2aef6b9f7c4ad9c031"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
